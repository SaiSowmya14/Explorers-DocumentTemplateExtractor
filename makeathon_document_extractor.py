# -*- coding: utf-8 -*-
"""Makeathon_Document_Extractor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RoBny7l3a1f9hzjOd31KQuksywKDqJva
"""

pip install pymupdf

"""**Step 1 :** DATA PREPROCESSING

**Step 1.1 :** Removing the Duplicate Files
"""

import os
import hashlib

def calculate_file_hash(file_path):
    """Calculates the hash value of a file's content."""
    hasher = hashlib.md5()
    with open(file_path, 'rb') as file:
        for chunk in iter(lambda: file.read(4096), b''):
            hasher.update(chunk)
    return hasher.hexdigest()

def find_duplicate_files(root_folder):
    """Traverses through the root folder and identifies duplicate files."""
    duplicates = {}
    for folder_path, _, file_names in os.walk(root_folder):
        for file_name in file_names:
            file_path = os.path.join(folder_path, file_name)
            file_hash = calculate_file_hash(file_path)
            if file_hash in duplicates:
                duplicates[file_hash].append(file_path)
            else:
                duplicates[file_hash] = [file_path]
    return duplicates

def remove_duplicate_files(duplicates):
    """Removes duplicate files from the file system."""
    count=0
    for file_paths in duplicates.values():
        if len(file_paths) > 1:
            count+=1
            print(f"Duplicate files found:\n{file_paths}\n")
            for file_path in file_paths[1:]:
                os.remove(file_path)
                print(f"{file_path} has been deleted.\n")
    if count==0:
      print("No Duplicate Files Found")

if __name__ == '__main__':
    root_folder = '/content/sample_data/PDF'
    duplicates = find_duplicate_files(root_folder)
    remove_duplicate_files(duplicates)

"""**Step 1.2 :** Extracting Text from PDF and moving the data to the List Format"""

import fitz

def extract_text_from_pdf(pdf_path):
  document = fitz.open(pdf_path)
  text = ""
  for page_num in range(document.page_count):
    page = document[page_num]
    text += page.get_text()
  return text

import os
pdf_lst  = os.listdir("/content/sample_data/PDF/")
print(pdf_lst)
pdf_list = ["//content/sample_data/PDF/"+s for s in pdf_lst]
print(pdf_list)

documents = [extract_text_from_pdf(path) for path in pdf_list]
print(documents[2])

import re
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

def preprocess_text(text):
  # Convert to lowercase
  text = text.lower()
  # Remove punctuation and digits
  text = re.sub(r'[\W]', ' ', text)
  return text

documents = [preprocess_text(doc) for doc in documents]

documents

"""**Step 2 :** Converting the Text Data into the Vector form by using TFIDFVectorizer

"""

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np

vectorizer = TfidfVectorizer()
Documents_vector = vectorizer.fit_transform(documents)
df = pd.DataFrame(Documents_vector.toarray(), columns=vectorizer.get_feature_names_out())
df.head()

"""**Step 3 :** Classification using SVM classifier"""

documents = [extract_text_from_pdf(path) for path in pdf_list]
d = {}
labels = []
for j in range(len(documents)):
  doc = list(documents[j].split('\n'))
  d[doc[0]] = d.get(doc[0],0)+1
  labels.append(doc[0])

print(labels)
print(d)

df['labels'] = labels
labels = np.array(labels).reshape(-1,1)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(Documents_vector,labels,test_size=0.2, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
svc = SVC(kernel='linear')
svc.fit(X_train,y_train)
y_pred = svc.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

"""**Step 4:** Clustering the Data using KMEANS clustering"""

d

from sklearn.cluster import KMeans
num_clusters=len(d)
kmeans=KMeans(n_clusters=num_clusters)
kmeans.fit(Documents_vector)
lables=kmeans.labels_

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
pca=PCA(n_components=2)
X_pca=pca.fit_transform(Documents_vector.toarray())
plt.scatter(X_pca[:,0],X_pca[:,1],c=lables,linewidths=1)
plt.title("Document Clustering")
plt.xlabel("component 1")
plt.ylabel("component 2")

lables

df

documents=[extract_text_from_pdf(path) for path in pdf_list]
print(documents[2])

"""**Step 5 :** Reverse Engineering to extract the template"""

import fitz
def remove_text_from_pdf(input_pdf,output_pdf,text_to_remove):
  doc=fitz.open(input_pdf)
  for page_num in range(len(doc)):
    page=doc.load_page(page_num)
    for i in text_to_remove:
      text_instances=page.search_for(i)
      for inst in text_instances:
        rect=inst
        page.add_redact_annot(rect)
        page.apply_redactions()
  doc.save(output_pdf)
  doc.close()

def reverse_eng(lable, documents, pdf_list, output1,maximum):
  l=list(df.index[df['labels']==lable])
  p=list(documents[12].split())
  d1={}
  for j in l:
    p=list(documents[j].split('\n'))
    for i in p:
      if i!='\ue00b' and i!='':
        d1[i]=d1.get(i,0)+1
  #print(d1)
  maxi=maximum
  check_list=[]
  not_check_list=[]
  for i in d1:
    if d1[i]>maximum:
      check_list.append(i)
    else:
      not_check_list.append(i)
  #print(check_list)
  #print(not_check_list)
  count=0
  index=0
  maximum=0
  for j in l:
    p=list(documents[j].split('\n'))
    #print(p)
    for i in p:
      if i in check_list:
        count += 1
    if count==0 or count>maximum:
      maximum=count
      index=j
    count=0
  #print(index)
  input_pdf=pdf_list[index]
  output_pdf="/content/sample_data/PDF/Output/"+output1+".pdf"
  remove_text_from_pdf(input_pdf,output_pdf,not_check_list)
  #print(output_pdf)
  #print(d1)

d

reverse_eng('Deposit Receipt', documents, pdf_list, "deposit_receipt",16)
reverse_eng('Renewal Notice', documents, pdf_list, "renewal_notice",16)
reverse_eng('Initial Premium Receipt', documents, pdf_list, "Initial_Premium_Receipt",12)
reverse_eng('Loan Repayment Receipt', documents, pdf_list, "Loan_Repayment_Receipt",1)